#!/usr/bin/env python3
"""
ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì •ì œí•˜ê³  ë²¡í„° DBì— ìµœì í™”
"""

import re
from pathlib import Path
from typing import List, Dict

class MarkdownCleaner:
    def __init__(self, input_path: Path):
        self.input_path = input_path
        self.sections = []
        
    def clean(self) -> str:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì •ì œ"""
        print("ğŸ§¹ ë§ˆí¬ë‹¤ìš´ ì •ì œ ì‹œì‘...")
        
        with open(self.input_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # 1. ì¤‘ë³µ ë¬¸ì ì œê±°
        content = self._fix_duplicated_chars(content)
        
        # 2. ë¹ˆ ì¤„ ì •ë¦¬
        content = self._clean_empty_lines(content)
        
        # 3. ì„¹ì…˜ë³„ë¡œ ë¶„í• 
        sections = self._split_into_sections(content)
        
        # 4. ê° ì„¹ì…˜ ì •ì œ
        cleaned_sections = []
        for section in sections:
            cleaned = self._clean_section(section)
            if cleaned:
                cleaned_sections.append(cleaned)
        
        return "\n\n".join(cleaned_sections)
    
    def _fix_duplicated_chars(self, text: str) -> str:
        """ì¤‘ë³µëœ ë¬¸ì ìˆ˜ì •"""
        # AA JJoouurrnneeyy -> A Journey
        patterns = [
            (r'AA JJoouurrnneeyy TT oowwaarrddss', 'A Journey Towards'),
            (r'aa SSuussttaa?ii?nnaabbllee FFuuttuurree', 'a Sustainable Future'),
            (r'ì‚¼ì‚¼ì„±ì„±ì „ì „ìì', 'ì‚¼ì„±ì „ì'),
            (r'ì§€ì§€ì†ì†ê°€ê°€ëŠ¥ëŠ¥ê²½ê²½ì˜ì˜', 'ì§€ì†ê°€ëŠ¥ê²½ì˜'),
            (r'ë³´ë³´ê³ ê³ ì„œì„œ', 'ë³´ê³ ì„œ'),
        ]
        
        for pattern, replacement in patterns:
            text = re.sub(pattern, replacement, text)
        
        # ì¼ë°˜ì ì¸ ì¤‘ë³µ ë¬¸ì íŒ¨í„´ ì œê±° (ê°™ì€ ë¬¸ìê°€ 2ë²ˆ ì´ìƒ ë°˜ë³µ)
        text = re.sub(r'([ê°€-í£])\1{2,}', r'\1', text)
        
        return text
    
    def _clean_empty_lines(self, text: str) -> str:
        """ë¹ˆ ì¤„ ì •ë¦¬"""
        # 3ì¤„ ì´ìƒì˜ ë¹ˆ ì¤„ì„ 2ì¤„ë¡œ
        text = re.sub(r'\n{4,}', '\n\n\n', text)
        return text
    
    def _split_into_sections(self, content: str) -> List[str]:
        """í˜ì´ì§€ë³„ë¡œ ì„¹ì…˜ ë¶„í• """
        # í˜ì´ì§€ êµ¬ë¶„ìë¡œ ë¶„í• 
        sections = re.split(r'---\n## ğŸ“„ í˜ì´ì§€ \d+\n', content)
        
        # ê° ì„¹ì…˜ì— í˜ì´ì§€ ë²ˆí˜¸ ë‹¤ì‹œ ì¶”ê°€
        page_headers = re.findall(r'---\n## ğŸ“„ í˜ì´ì§€ (\d+)\n', content)
        
        result = []
        for i, section in enumerate(sections[1:]):  # ì²« ë²ˆì§¸ëŠ” í—¤ë”ì´ë¯€ë¡œ ì œì™¸
            if i < len(page_headers):
                result.append(f"## ğŸ“„ í˜ì´ì§€ {page_headers[i]}\n{section}")
        
        return result
    
    def _clean_section(self, section: str) -> str:
        """ê° ì„¹ì…˜ ì •ì œ"""
        lines = section.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            
            # ë¹ˆ ì¤„ ìŠ¤í‚µ
            if not line:
                continue
            
            # ë„ˆë¬´ ì§§ì€ ì¤„ì€ ì œê±° (ì¡ìŒ)
            if len(line) < 3 and not line.isdigit():
                continue
            
            # í˜ì´ì§€ ë²ˆí˜¸ë§Œ ìˆëŠ” ì¤„ ì œê±°
            if re.match(r'^\d{1,3}$', line):
                continue
            
            # ì •ì œëœ ì¤„ ì¶”ê°€
            cleaned_lines.append(line)
        
        # ì—°ì†ëœ ì§§ì€ ì¤„ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        merged_lines = self._merge_short_lines(cleaned_lines)
        
        return '\n'.join(merged_lines)
    
    def _merge_short_lines(self, lines: List[str]) -> List[str]:
        """ì§§ì€ ì¤„ë“¤ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ í•©ì¹˜ê¸°"""
        merged = []
        buffer = []
        
        for line in lines:
            # ì œëª©ì´ë‚˜ í—¤ë”ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            if line.startswith('#') or line.startswith('**['):
                if buffer:
                    merged.append(' '.join(buffer))
                    buffer = []
                merged.append(line)
            # ë¦¬ìŠ¤íŠ¸ í•­ëª©ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
            elif line.startswith('-') or line.startswith('â€¢') or re.match(r'^\d+\.', line):
                if buffer:
                    merged.append(' '.join(buffer))
                    buffer = []
                merged.append(line)
            # í‘œ êµ¬ë¶„ìëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            elif '|' in line:
                if buffer:
                    merged.append(' '.join(buffer))
                    buffer = []
                merged.append(line)
            # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ë²„í¼ì— ì¶”ê°€
            else:
                if len(line) < 50:  # ì§§ì€ ì¤„ì€ ë²„í¼ì— ì¶”ê°€
                    buffer.append(line)
                else:  # ê¸´ ì¤„ì€ ê·¸ëŒ€ë¡œ ì¶”ê°€
                    if buffer:
                        merged.append(' '.join(buffer))
                        buffer = []
                    merged.append(line)
        
        # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
        if buffer:
            merged.append(' '.join(buffer))
        
        return merged
    
    def create_chunks(self, content: str) -> List[Dict]:
        """ë²¡í„° DBë¥¼ ìœ„í•œ ì²­í¬ ìƒì„±"""
        chunks = []
        sections = content.split('## ğŸ“„ í˜ì´ì§€')
        
        for section in sections[1:]:  # ì²« ë²ˆì§¸ëŠ” ë¹ˆ ë¬¸ìì—´
            # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
            page_match = re.match(r' (\d+)\n', section)
            if not page_match:
                continue
            
            page_num = page_match.group(1)
            section_content = section[page_match.end():]
            
            # ì„¹ì…˜ì„ ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í•  (500ì ë‹¨ìœ„)
            chunk_size = 500
            for i in range(0, len(section_content), chunk_size):
                chunk_text = section_content[i:i+chunk_size]
                
                if len(chunk_text.strip()) > 50:  # ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ë§Œ
                    chunks.append({
                        'content': chunk_text.strip(),
                        'metadata': {
                            'page': int(page_num),
                            'chunk_start': i,
                            'chunk_end': min(i+chunk_size, len(section_content))
                        }
                    })
        
        return chunks

def main():
    # ê²½ë¡œ ì„¤ì •
    input_path = Path("/Users/donghyunkim/Desktop/joo_project/samsung_chatbot/data/samsung_esg_processed.md")
    output_path = Path("/Users/donghyunkim/Desktop/joo_project/samsung_chatbot/data/samsung_esg_cleaned.md")
    
    # ì •ì œ ì‹¤í–‰
    cleaner = MarkdownCleaner(input_path)
    cleaned_content = cleaner.clean()
    
    # íŒŒì¼ ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# ì‚¼ì„±ì „ì ì§€ì†ê°€ëŠ¥ê²½ì˜ ë³´ê³ ì„œ 2025 (ì •ì œë¨)\n")
        f.write("*êµ¬ì¡°í™”ë˜ê³  ì •ì œëœ í…ìŠ¤íŠ¸ - RAG ìµœì í™”*\n\n")
        f.write("---\n\n")
        f.write(cleaned_content)
    
    print(f"âœ… ì •ì œëœ ë§ˆí¬ë‹¤ìš´ ì €ì¥: {output_path}")
    
    # ì²­í¬ ìƒì„±
    chunks = cleaner.create_chunks(cleaned_content)
    print(f"ğŸ“Š ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    
    # ìƒ˜í”Œ ì¶œë ¥
    print("\nğŸ“ ì²­í¬ ìƒ˜í”Œ:")
    for i, chunk in enumerate(chunks[:3]):
        print(f"\nì²­í¬ {i+1} (í˜ì´ì§€ {chunk['metadata']['page']}):")
        print(f"  {chunk['content'][:100]}...")
    
    return output_path, chunks

if __name__ == "__main__":
    output_file, chunks = main()
    print(f"\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„: ì •ì œëœ íŒŒì¼ì„ ë²¡í„° DBë¡œ ë³€í™˜í•˜ì„¸ìš”.")