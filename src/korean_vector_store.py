#!/usr/bin/env python3
"""
í•œêµ­ì–´ íŠ¹í™” ë²¡í„° ìŠ¤í† ì–´
ko-sroberta-multitask ëª¨ë¸ì„ ì‚¬ìš©í•œ í•œêµ­ì–´ ì„ë² ë”© ìµœì í™”
"""

from typing import List, Dict, Optional

# SQLite ë²„ì „ íŒ¨ì¹˜ for Streamlit Cloud
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass  # Local environment doesn't need this patch

import chromadb
from chromadb.config import Settings
from langchain.schema import Document
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
import os

class KoreanVectorStore:
    def __init__(self, persist_directory: str):
        self.persist_directory = persist_directory
        
        # ko-sroberta-multitask ëª¨ë¸ ë¡œë“œ
        print("ğŸ”„ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.tokenizer = AutoTokenizer.from_pretrained("jhgan/ko-sroberta-multitask")
        self.model = AutoModel.from_pretrained("jhgan/ko-sroberta-multitask")
        self.model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = None
        self.collection = None
        self._initialize_store()
        
        # ì˜ì–´ ì•½ì–´ ë§¤í•‘ í…Œì´ë¸”
        self.abbreviation_map = {
            "DX": "DX(ë””ë°”ì´ìŠ¤ê²½í—˜ë¶€ë¬¸)",
            "DS": "DS(ë””ë°”ì´ìŠ¤ì†”ë£¨ì…˜ë¶€ë¬¸)",
            "CEO": "CEO(ìµœê³ ê²½ì˜ì)",
            "ESG": "ESG(í™˜ê²½ì‚¬íšŒê±°ë²„ë„ŒìŠ¤)",
            "SDGs": "SDGs(ì§€ì†ê°€ëŠ¥ë°œì „ëª©í‘œ)",
            "AWS": "AWS(êµ­ì œìˆ˜ìì›ê´€ë¦¬ë™ë§¹)",
            "TCFD": "TCFD(ê¸°í›„ë³€í™”ì¬ë¬´ì •ë³´ê³µê°œ)",
            "CPMS": "CPMS(ì»´í”Œë¼ì´ì–¸ìŠ¤í”„ë¡œê·¸ë¨ê´€ë¦¬ì‹œìŠ¤í…œ)",
            "RBA": "RBA(ì±…ì„ìˆëŠ”ë¹„ì¦ˆë‹ˆìŠ¤ì—°í•©)",
            "Scope 1": "Scope 1(ì§ì ‘ë°°ì¶œ)",
            "Scope 2": "Scope 2(ê°„ì ‘ë°°ì¶œ)",
            "Scope 3": "Scope 3(ê¸°íƒ€ê°„ì ‘ë°°ì¶œ)",
            "AI": "AI(ì¸ê³µì§€ëŠ¥)",
            "SW": "SW(ì†Œí”„íŠ¸ì›¨ì–´)",
            "R&D": "R&D(ì—°êµ¬ê°œë°œ)",
            "M&A": "M&A(ì¸ìˆ˜í•©ë³‘)",
            "NPU": "NPU(ì‹ ê²½ë§ì²˜ë¦¬ì¥ì¹˜)",
            "GPU": "GPU(ê·¸ë˜í”½ì²˜ë¦¬ì¥ì¹˜)",
            "CPU": "CPU(ì¤‘ì•™ì²˜ë¦¬ì¥ì¹˜)"
        }
        
        # ë™ì˜ì–´ ë§¤í•‘
        self.synonym_map = {
            "ë§¤ì¶œ": ["ë§¤ì¶œ", "ìˆ˜ìµ", "ì‹¤ì ", "ë§¤ì¶œì•¡", "ì˜ì—…ìˆ˜ìµ"],
            "ì´ìµ": ["ì´ìµ", "ì˜ì—…ì´ìµ", "ìˆœì´ìµ", "ìˆ˜ìµì„±"],
            "í™˜ê²½": ["í™˜ê²½", "ì¹œí™˜ê²½", "ì§€ì†ê°€ëŠ¥", "ESG", "ê·¸ë¦°"],
            "íƒ„ì†Œ": ["íƒ„ì†Œ", "ì˜¨ì‹¤ê°€ìŠ¤", "CO2", "ì´ì‚°í™”íƒ„ì†Œ", "ë°°ì¶œëŸ‰"],
            "ì¬ìƒì—ë„ˆì§€": ["ì¬ìƒì—ë„ˆì§€", "ì‹ ì¬ìƒì—ë„ˆì§€", "ì¬ìƒê°€ëŠ¥ì—ë„ˆì§€", "íƒœì–‘ê´‘", "í’ë ¥"],
            "íê¸°ë¬¼": ["íê¸°ë¬¼", "ì“°ë ˆê¸°", "íì œí’ˆ", "ì¬í™œìš©", "ìˆœí™˜ìì›"],
            "ì„ì§ì›": ["ì„ì§ì›", "ì§ì›", "ì¢…ì—…ì›", "ê·¼ë¡œì", "ì¸ë ¥"],
            "í˜‘ë ¥ì‚¬": ["í˜‘ë ¥ì‚¬", "í˜‘ë ¥íšŒì‚¬", "ê³µê¸‰ì—…ì²´", "íŒŒíŠ¸ë„ˆì‚¬", "ë²¤ë”"]
        }
    
    def _initialize_store(self):
        """ChromaDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        self.client = chromadb.PersistentClient(path=self.persist_directory)
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
        try:
            self.collection = self.client.get_collection("samsung_esg_korean")
            doc_count = self.collection.count()
            if doc_count > 0:
                print(f"âœ… ê¸°ì¡´ ChromaDB ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ ({doc_count}ê°œ ë¬¸ì„œ)")
            else:
                print("âš ï¸ ë¹ˆ ì»¬ë ‰ì…˜ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except:
            # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            self.collection = self.client.create_collection(
                name="samsung_esg_korean",
                metadata={"description": "ì‚¼ì„±ì „ì ESG ë³´ê³ ì„œ - í•œêµ­ì–´ ìµœì í™”"}
            )
            print("âœ… ìƒˆ ChromaDB ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì˜ì–´ ì•½ì–´ë¥¼ í•œê¸€ ë³‘ê¸°ë¡œ ë³€í™˜"""
        # ì˜ì–´ ì•½ì–´ ì²˜ë¦¬
        for eng, kor in self.abbreviation_map.items():
            # ì´ë¯¸ ë³‘ê¸°ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ë³€í™˜
            if eng in text and kor not in text:
                text = text.replace(eng, kor)
        
        return text
    
    def enhance_query(self, query: str) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥ - ë™ì˜ì–´ ì¶”ê°€"""
        enhanced = query
        
        # ë™ì˜ì–´ í™•ì¥
        for key, synonyms in self.synonym_map.items():
            if key in query:
                # ë™ì˜ì–´ë¥¼ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
                for synonym in synonyms:
                    if synonym not in enhanced:
                        enhanced += f" {synonym}"
        
        return enhanced
    
    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """ko-srobertaë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_texts = [self.preprocess_text(text) for text in texts]
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            processed_texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        )
        
        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            outputs = self.model(**inputs)
            # [CLS] í† í°ì˜ hidden stateë¥¼ ì‚¬ìš©
            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        
        return embeddings
    
    def add_documents(self, texts: List[str], metadatas: List[Dict]):
        """ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì¶”ê°€"""
        if not texts:
            return
        
        print(f"ğŸ“ {len(texts)}ê°œ ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì¶”ê°€ ì¤‘...")
        
        # ID ìƒì„±
        ids = [f"doc_{i:04d}" for i in range(len(texts))]
        
        # ë°°ì¹˜ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        batch_size = 50  # ko-srobertaëŠ” ë¬´ê±°ìš°ë¯€ë¡œ ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ
        
        for i in range(0, len(texts), batch_size):
            batch_end = min(i + batch_size, len(texts))
            batch_texts = texts[i:batch_end]
            batch_metadata = metadatas[i:batch_end]
            batch_ids = ids[i:batch_end]
            
            # ì„ë² ë”© ìƒì„±
            print(f"  ë°°ì¹˜ {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1} ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = self.get_embeddings(batch_texts)
            
            # ChromaDBì— ì¶”ê°€
            self.collection.add(
                embeddings=embeddings.tolist(),
                documents=batch_texts,
                metadatas=batch_metadata,
                ids=batch_ids
            )
        
        # ChromaDB PersistentClientëŠ” ìë™ìœ¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ ë³„ë„ persist ë¶ˆí•„ìš”
        print(f"âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {self.persist_directory}")
    
    def similarity_search(
        self, 
        query: str, 
        k: int = 10,
        filter: Optional[Dict] = None
    ) -> List[Document]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        
        # ì¿¼ë¦¬ í™•ì¥
        enhanced_query = self.enhance_query(query)
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.get_embeddings([enhanced_query])[0]
        
        # ê²€ìƒ‰ ì‹¤í–‰
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=k,
            where=filter  # ë©”íƒ€ë°ì´í„° í•„í„°ë§
        )
        
        # Document ê°ì²´ë¡œ ë³€í™˜
        documents = []
        if results['documents'] and results['documents'][0]:
            for doc, metadata, distance in zip(
                results['documents'][0], 
                results['metadatas'][0],
                results['distances'][0] if 'distances' in results else [0] * len(results['documents'][0])
            ):
                metadata['distance'] = distance  # ê±°ë¦¬ ì •ë³´ ì¶”ê°€
                documents.append(Document(
                    page_content=doc,
                    metadata=metadata
                ))
        
        return documents
    
    def search_with_context(self, query: str, k: int = 5) -> List[Document]:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ê²€ìƒ‰ (ì•ë’¤ ì²­í¬ í¬í•¨)"""
        # ê¸°ë³¸ ê²€ìƒ‰
        docs = self.similarity_search(query, k=k)
        
        # ê° ë¬¸ì„œì˜ ì•ë’¤ ì²­í¬ë„ ê°€ì ¸ì˜¤ê¸°
        extended_docs = []
        added_ids = set()
        
        for doc in docs:
            chunk_id = doc.metadata.get('chunk_id', '')
            if chunk_id and chunk_id not in added_ids:
                # í˜„ì¬ ì²­í¬
                extended_docs.append(doc)
                added_ids.add(chunk_id)
                
                # TODO: ì•ë’¤ ì²­í¬ ê°€ì ¸ì˜¤ê¸° ë¡œì§ êµ¬í˜„
                # ì´ëŠ” chunk_id ì²´ê³„ì— ë”°ë¼ êµ¬í˜„ í•„ìš”
        
        return extended_docs
    
    def exists(self) -> bool:
        """ë²¡í„° ìŠ¤í† ì–´ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        try:
            count = self.collection.count()
            return count > 0
        except:
            return False
    
    def clear(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        if self.collection:
            # ëª¨ë“  ë¬¸ì„œ ì‚­ì œ
            all_ids = self.collection.get()['ids']
            if all_ids:
                self.collection.delete(ids=all_ids)
            print("ğŸ—‘ï¸ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_statistics(self) -> Dict:
        """ë²¡í„° DB í†µê³„ ì •ë³´"""
        if not self.collection:
            return {}
        
        data = self.collection.get()
        
        # ë©”íƒ€ë°ì´í„° ë¶„ì„
        sections = set()
        pages = set()
        chunk_types = {}
        
        for metadata in data.get('metadatas', []):
            if 'section' in metadata:
                sections.add(metadata['section'])
            if 'page' in metadata:
                pages.add(metadata['page'])
            if 'chunk_type' in metadata:
                chunk_type = metadata['chunk_type']
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
        
        return {
            'total_documents': len(data.get('ids', [])),
            'unique_sections': len(sections),
            'unique_pages': len(pages),
            'chunk_types': chunk_types,
            'embedding_dimension': 768  # ko-sroberta dimension
        }


# VectorStore í´ë˜ìŠ¤ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ í˜¸í™˜)
class VectorStore(KoreanVectorStore):
    """ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤"""
    
    def add_documents(self, texts: List[str], metadatas: List[Dict] = None):
        """ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€"""
        if metadatas is None:
            metadatas = [{}] * len(texts)
        super().add_documents(texts, metadatas)