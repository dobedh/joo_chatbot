from typing import List, Dict, Optional
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.chains import ConversationalRetrievalChain
import os
import google.generativeai as genai
from .hybrid_search import HybridSearch

class GeminiRAGPipeline:
    def __init__(self, vector_store, model_name: str = "gemini-2.0-flash-exp", temperature: float = 0.7):
        self.vector_store = vector_store
        self.model_name = model_name
        self.temperature = temperature
        
        # Configure Gemini API
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        
        # Initialize LLM
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            temperature=temperature,
            google_api_key=os.getenv("GOOGLE_API_KEY"),
            max_output_tokens=2000
        )
        
        # Initialize memory
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # Create prompt template
        self.prompt_template = self._create_prompt_template()
        
        # Initialize hybrid search
        self.hybrid_search = HybridSearch(vector_store, dense_weight=0.6)
        
        # Initialize chain
        self.chain = None
        self._initialize_chain()
    
    def _create_prompt_template(self) -> PromptTemplate:
        """Create the prompt template for the chatbot"""
        template = """ë‹¹ì‹ ì€ ì‚¼ì„±ì „ìž ì§€ì†ê°€ëŠ¥ê²½ì˜ ë³´ê³ ì„œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. 
        ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        
        ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”:
        1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
        2. ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ë¶ˆí™•ì‹¤í•œ ê²½ìš° ê·¸ë ‡ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”
        3. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì‚¬ë¡€ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
        4. ë‹µë³€ì€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ìž‘ì„±í•´ì£¼ì„¸ìš”
        5. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”
        
        ì°¸ê³  ë¬¸ì„œ:
        {context}
        
        ëŒ€í™” ê¸°ë¡:
        {chat_history}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:"""
        
        return PromptTemplate(
            template=template,
            input_variables=["context", "chat_history", "question"]
        )
    
    def _create_retriever(self):
        """Create a custom retriever for our vector store"""
        class CustomRetriever:
            def __init__(self, vector_store):
                self.vector_store = vector_store
            
            def get_relevant_documents(self, query: str) -> List[Document]:
                return self.vector_store.similarity_search(query, k=5)
        
        return CustomRetriever(self.vector_store)
    
    def _initialize_chain(self):
        """Initialize the conversational retrieval chain"""
        if not self.vector_store or not self.vector_store.exists():
            raise ValueError("Vector store is not initialized or empty")
        
        # No longer need traditional retriever since we use hybrid search
        self.retriever = self._create_retriever()  # Keep for compatibility
    
    def query(self, question: str) -> Dict:
        """Process a query and return response with sources"""
        if not self.retriever:
            return {
                "answer": "ì‹œìŠ¤í…œì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PDFë¥¼ ë¨¼ì € ì²˜ë¦¬í•´ì£¼ì„¸ìš”.",
                "sources": []
            }
        
        try:
            # Use hybrid search instead of simple retriever
            docs = self.hybrid_search.search(question, k=5)
            
            # ë””ë²„ê¹…ìš©: ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥
            print(f"\nðŸ” [ë””ë²„ê¹…] ê²€ìƒ‰ ì¿¼ë¦¬: {question}")
            print(f"ðŸ“š [ë””ë²„ê¹…] ê²€ìƒ‰ëœ {len(docs)}ê°œ ë¬¸ì„œ:")
            for i, doc in enumerate(docs, 1):
                print(f"  [{i}] íŽ˜ì´ì§€ {doc.metadata.get('page', 'N/A')}, "
                      f"ì„¹ì…˜: {doc.metadata.get('section', 'N/A')}, "
                      f"íƒ€ìž…: {doc.metadata.get('chunk_type', 'N/A')}")
                print(f"      ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:100]}...")
            print("-" * 60)
            
            if not docs:
                return {
                    "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.",
                    "sources": []
                }
            
            # Create context from documents
            context = "\n\n".join([doc.page_content for doc in docs])
            
            # Get chat history
            chat_history = ""
            if self.memory.chat_memory.messages:
                history_messages = []
                for msg in self.memory.chat_memory.messages[-4:]:  # Last 4 messages
                    role = "ì‚¬ìš©ìž" if msg.type == "human" else "ì–´ì‹œìŠ¤í„´íŠ¸"
                    history_messages.append(f"{role}: {msg.content}")
                chat_history = "\n".join(history_messages)
            
            # Format prompt
            prompt = self.prompt_template.format(
                context=context,
                chat_history=chat_history,
                question=question
            )
            
            # Get response from LLM
            response = self.llm.predict(prompt)
            
            # Update memory
            self.memory.chat_memory.add_user_message(question)
            self.memory.chat_memory.add_ai_message(response)
            
            # Format source documents with full metadata
            sources = []
            for i, doc in enumerate(docs, 1):
                sources.append({
                    "index": i,
                    "page": doc.metadata.get("page", "Unknown"),
                    "section": doc.metadata.get("section", "Unknown"),
                    "chunk_type": doc.metadata.get("chunk_type", "Unknown"),
                    "content": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                    "keywords": doc.metadata.get("keywords", ""),
                    "metrics": doc.metadata.get("metrics", "")
                })
            
            return {
                "answer": response,
                "sources": sources
            }
        
        except Exception as e:
            return {
                "answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": []
            }
    
    def clear_memory(self):
        """Clear conversation memory"""
        self.memory.clear()
        print("Conversation memory cleared")
    
    def get_conversation_history(self) -> List[Dict]:
        """Get conversation history"""
        messages = self.memory.chat_memory.messages
        history = []
        
        for i in range(0, len(messages), 2):
            if i + 1 < len(messages):
                history.append({
                    "question": messages[i].content,
                    "answer": messages[i + 1].content
                })
        
        return history