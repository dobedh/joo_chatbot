#!/usr/bin/env python3
"""
ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ëª¨ë“ˆ
ì˜ë¯¸ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ê³  ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œ
"""

import re
from typing import List, Dict, Tuple
from pathlib import Path


class SmartChunker:
    def __init__(self, chunk_size: int = 1200, chunk_overlap: int = 300):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # ì£¼ìš” ì„¹ì…˜ í‚¤ì›Œë“œ
        self.section_keywords = {
            'DXë¶€ë¬¸': ['DX', 'Device eXperience', 'ëª¨ë°”ì¼', 'TV', 'ê°€ì „'],
            'DSë¶€ë¬¸': ['DS', 'Device Solutions', 'ë°˜ë„ì²´', 'ë©”ëª¨ë¦¬', 'Foundry'],
            'í™˜ê²½': ['í™˜ê²½', 'ê¸°í›„ë³€í™”', 'íƒ„ì†Œì¤‘ë¦½', 'ì¬ìƒì—ë„ˆì§€', 'ìì›ìˆœí™˜', 'ìˆ˜ìì›'],
            'ì‚¬íšŒ': ['ì„ì§ì›', 'ê³µê¸‰ë§', 'ì‚¬íšŒê³µí—Œ', 'ì¸ê¶Œ', 'ì•ˆì „ë³´ê±´'],
            'ê±°ë²„ë„ŒìŠ¤': ['ì´ì‚¬íšŒ', 'ì§€ë°°êµ¬ì¡°', 'ì¤€ë²•', 'ìœ¤ë¦¬ê²½ì˜', 'ì»´í”Œë¼ì´ì–¸ìŠ¤']
        }
        
        # ì¤‘ìš” ìˆ˜ì¹˜ íŒ¨í„´
        self.number_patterns = [
            r'\d+ì¡°\s*\d*[ì²œë°±ì‹­ì–µë§Œ]*ì›',  # ì¡° ë‹¨ìœ„ ê¸ˆì•¡
            r'\d+ì–µ\s*\d*[ì²œë°±ì‹­ë§Œ]*ì›',    # ì–µ ë‹¨ìœ„ ê¸ˆì•¡
            r'\d+\.?\d*%',                 # í¼ì„¼íŠ¸
            r'\d{4}ë…„',                    # ì—°ë„
            r'\d+ë§Œ\s*í†¤',                 # í†¤ ë‹¨ìœ„
            r'\d+[ì²œë°±ì‹­ë§Œ]*ëª…',            # ì¸ì›ìˆ˜
        ]
    
    def chunk_markdown(self, markdown_path: Path) -> List[Dict]:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì²­í‚¹"""
        with open(markdown_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        chunks = []
        
        # í˜ì´ì§€ë³„ë¡œ ë¶„í• 
        pages = self._split_by_pages(content)
        
        for page_num, page_content in pages:
            # ê° í˜ì´ì§€ë¥¼ ì„¹ì…˜ë³„ë¡œ ë¶„í• 
            page_chunks = self._chunk_page(page_content, page_num)
            chunks.extend(page_chunks)
        
        return chunks
    
    def _split_by_pages(self, content: str) -> List[Tuple[int, str]]:
        """í˜ì´ì§€ë³„ë¡œ ë¶„í• """
        pages = []
        
        # í˜ì´ì§€ êµ¬ë¶„ìë¡œ ë¶„í• 
        page_splits = re.split(r'## í˜ì´ì§€ (\d+)', content)
        
        # ì²« ë²ˆì§¸ ìš”ì†ŒëŠ” í—¤ë”ì´ë¯€ë¡œ ì œì™¸
        for i in range(1, len(page_splits), 2):
            if i + 1 < len(page_splits):
                page_num = int(page_splits[i])
                page_content = page_splits[i + 1]
                pages.append((page_num, page_content))
        
        return pages
    
    def _chunk_page(self, page_content: str, page_num: int) -> List[Dict]:
        """í˜ì´ì§€ ë‚´ìš©ì„ ì²­í‚¹"""
        chunks = []
        
        # í‘œ ë°ì´í„° ì„¹ì…˜ ì°¾ê¸°
        table_sections = re.findall(
            r'### ğŸ“Š ì£¼ìš” ë°ì´í„°\n```(.*?)```', 
            page_content, 
            re.DOTALL
        )
        
        # í‘œ ë°ì´í„°ëŠ” í†µì§¸ë¡œ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
        for table in table_sections:
            if table.strip():
                chunk_data = {
                    'content': table.strip(),
                    'metadata': {
                        'page': page_num,
                        'chunk_type': 'table',
                        'section': self._detect_section(page_content),
                        'metrics': self._extract_numbers(table),
                        'keywords': self._extract_keywords(table)
                    }
                }
                chunks.append(chunk_data)
        
        # í‘œë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        text_without_tables = re.sub(
            r'### ğŸ“Š ì£¼ìš” ë°ì´í„°\n```.*?```', 
            '', 
            page_content, 
            flags=re.DOTALL
        )
        
        # ì„¹ì…˜ë³„ë¡œ ë¶„í•  (### ê¸°ì¤€)
        sections = re.split(r'###\s+', text_without_tables)
        
        for section in sections:
            if not section.strip():
                continue
            
            # ì„¹ì…˜ ì œëª©ê³¼ ë‚´ìš© ë¶„ë¦¬
            lines = section.split('\n', 1)
            section_title = lines[0].strip() if lines else ''
            section_content = lines[1] if len(lines) > 1 else section
            
            # ì„¹ì…˜ ë‚´ìš©ì„ ì²­í¬ë¡œ ë¶„í• 
            if section_content.strip():
                section_chunks = self._create_chunks_from_text(
                    section_content,
                    page_num,
                    section_title
                )
                chunks.extend(section_chunks)
        
        return chunks
    
    def _create_chunks_from_text(
        self, 
        text: str, 
        page_num: int,
        section_title: str = ''
    ) -> List[Dict]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        
        # ë¬¸ë‹¨ë³„ë¡œ ë¶„ë¦¬
        paragraphs = re.split(r'\n\n+', text)
        
        current_chunk = []
        current_length = 0
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            para_length = len(para)
            
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í• ì§€ ê²°ì •
            if current_length + para_length <= self.chunk_size:
                current_chunk.append(para)
                current_length += para_length
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunk_text = '\n\n'.join(current_chunk)
                    chunks.append(self._create_chunk_data(
                        chunk_text, 
                        page_num, 
                        section_title
                    ))
                
                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = [para]
                current_length = para_length
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunk_text = '\n\n'.join(current_chunk)
            chunks.append(self._create_chunk_data(
                chunk_text, 
                page_num, 
                section_title
            ))
        
        return chunks
    
    def _create_chunk_data(
        self, 
        text: str, 
        page_num: int,
        section_title: str = ''
    ) -> Dict:
        """ì²­í¬ ë°ì´í„° ìƒì„±"""
        # ì„¹ì…˜ ê°ì§€
        section = self._detect_section(text)
        
        # ì²­í¬ íƒ€ì… ê²°ì •
        chunk_type = self._detect_chunk_type(text)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            'page': page_num,
            'section': section,
            'subsection': section_title,
            'chunk_type': chunk_type,
            'metrics': self._extract_numbers(text),
            'keywords': self._extract_keywords(text),
            'char_count': len(text)
        }
        
        # ì—°ë„ ì •ë³´ ì¶”ì¶œ
        years = re.findall(r'(20\d{2})ë…„', text)
        if years:
            metadata['years'] = list(set(years))
        
        return {
            'content': text,
            'metadata': metadata
        }
    
    def _detect_section(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ê°€ ì†í•œ ì„¹ì…˜ ê°ì§€"""
        text_lower = text.lower()
        
        for section, keywords in self.section_keywords.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    return section
        
        return 'ì¼ë°˜'
    
    def _detect_chunk_type(self, text: str) -> str:
        """ì²­í¬ íƒ€ì… ê°ì§€"""
        # CEO ë©”ì‹œì§€ ê°ì§€
        if 'CEO ë©”ì‹œì§€' in text or 'ëŒ€í‘œì´ì‚¬' in text:
            return 'ceo_message'
        
        # ì œëª©/í—¤ë” ê°ì§€ (ì§§ê³  ê°œí–‰ì´ ì—†ìŒ)
        if len(text) < 100 and '\n' not in text:
            return 'header'
        
        # ë¦¬ìŠ¤íŠ¸ ê°ì§€
        if text.count('â€¢') > 2 or text.count('Â·') > 2:
            return 'list'
        
        # ìˆ«ìê°€ ë§ìœ¼ë©´ ë°ì´í„°
        numbers = re.findall(r'\d+[ì¡°ì–µë§Œì²œë°±ì‹­]?\s*[ì›í†¤ëª…ê°œ%]', text)
        if len(numbers) > 3:
            return 'data'
        
        return 'text'
    
    def _extract_numbers(self, text: str) -> List[str]:
        """ì¤‘ìš” ìˆ˜ì¹˜ ì¶”ì¶œ"""
        numbers = []
        
        for pattern in self.number_patterns:
            matches = re.findall(pattern, text)
            numbers.extend(matches)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        return list(set(numbers))
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ì •ì˜ëœ ì¤‘ìš” í‚¤ì›Œë“œ (í™•ì¥)
        important_terms = [
            'íƒ„ì†Œì¤‘ë¦½', 'ì¬ìƒì—ë„ˆì§€', 'ESG', 'ì§€ì†ê°€ëŠ¥', 
            'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'í™˜ê²½', 'ì•ˆì „', 'ì¸ê¶Œ',
            'AI', 'ë°˜ë„ì²´', 'í˜ì‹ ', 'ë””ì§€í„¸', 'ê·¸ë¦°',
            'ìˆœí™˜ê²½ì œ', 'ìƒë¬¼ë‹¤ì–‘ì„±', 'ë„·ì œë¡œ',
            # ì¶”ê°€ í‚¤ì›Œë“œ
            'HRA', 'Human Rights Risk Assessment', 'ì¸ê¶Œ ë¦¬ìŠ¤í¬ í‰ê°€',
            'ì¸ê¶Œ ì±”í”¼ì–¸', 'ì¸ê¶Œ êµìœ¡', 'ìˆ˜ë£Œìœ¨', '95.7%',
            'DXë¶€ë¬¸', 'DSë¶€ë¬¸', 'Scope 1', 'Scope 2', 'Scope 3',
            'TCFD', 'SASB', 'GRI', 'CDP', 'UNGC', 'RBA',
            'ì˜¨ì‹¤ê°€ìŠ¤', 'CO2', 'ë°°ì¶œëŸ‰', 'ì¬í™œìš©', 'íê¸°ë¬¼'
        ]
        
        # ì˜ì–´ ì•½ì–´ íŒ¨í„´ ì¶”ì¶œ (ëŒ€ë¬¸ì 2ê¸€ì ì´ìƒ)
        import re
        acronyms = re.findall(r'\b[A-Z]{2,}\b', text)
        keywords.extend(acronyms)
        
        for term in important_terms:
            if term in text:
                keywords.append(term)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(keywords))
    
    def create_overlap_chunks(
        self, 
        text: str, 
        chunk_size: int = 500,
        overlap: int = 100
    ) -> List[str]:
        """ì˜¤ë²„ë©ì´ ìˆëŠ” ì²­í¬ ìƒì„±"""
        chunks = []
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size <= chunk_size:
                current_chunk.append(sentence)
                current_size += sentence_size
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                
                # ì˜¤ë²„ë© ì²˜ë¦¬ - ë§ˆì§€ë§‰ ëª‡ ë¬¸ì¥ì„ ë‹¤ìŒ ì²­í¬ì— í¬í•¨
                overlap_sentences = []
                overlap_size = 0
                
                for sent in reversed(current_chunk):
                    if overlap_size + len(sent) <= overlap:
                        overlap_sentences.insert(0, sent)
                        overlap_size += len(sent)
                    else:
                        break
                
                # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© í¬í•¨)
                current_chunk = overlap_sentences + [sentence]
                current_size = overlap_size + sentence_size
        
        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks


def test_chunker():
    """ì²­ì»¤ í…ŒìŠ¤íŠ¸"""
    markdown_path = Path("/Users/donghyunkim/Desktop/joo_project/samsung_chatbot/data/samsung_esg_advanced.md")
    
    if not markdown_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {markdown_path}")
        return
    
    chunker = SmartChunker(chunk_size=1200, chunk_overlap=300)
    chunks = chunker.chunk_markdown(markdown_path)
    
    print(f"ğŸ“Š ì²­í‚¹ ê²°ê³¼:")
    print(f"  ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
    
    # ì²­í¬ íƒ€ì…ë³„ í†µê³„
    chunk_types = {}
    sections = {}
    
    for chunk in chunks:
        chunk_type = chunk['metadata'].get('chunk_type', 'unknown')
        chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
        
        section = chunk['metadata'].get('section', 'unknown')
        sections[section] = sections.get(section, 0) + 1
    
    print(f"\nğŸ“ˆ ì²­í¬ íƒ€ì… ë¶„í¬:")
    for ctype, count in sorted(chunk_types.items(), key=lambda x: x[1], reverse=True):
        print(f"  {ctype}: {count}ê°œ")
    
    print(f"\nğŸ“‘ ì„¹ì…˜ë³„ ë¶„í¬:")
    for section, count in sorted(sections.items(), key=lambda x: x[1], reverse=True):
        print(f"  {section}: {count}ê°œ")
    
    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\nğŸ“ ìƒ˜í”Œ ì²­í¬ (ì²˜ìŒ 3ê°œ):")
    for i, chunk in enumerate(chunks[:3]):
        print(f"\n[ì²­í¬ {i+1}]")
        print(f"  í˜ì´ì§€: {chunk['metadata']['page']}")
        print(f"  ì„¹ì…˜: {chunk['metadata']['section']}")
        print(f"  íƒ€ì…: {chunk['metadata']['chunk_type']}")
        print(f"  ë‚´ìš©: {chunk['content'][:100]}...")
        if chunk['metadata'].get('metrics'):
            print(f"  ìˆ˜ì¹˜: {chunk['metadata']['metrics'][:3]}")


if __name__ == "__main__":
    test_chunker()