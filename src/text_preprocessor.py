#!/usr/bin/env python3
"""
í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ
í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ê°œì„ 
"""

import re
from typing import List, Dict, Optional, Tuple
import json


class TextPreprocessor:
    def __init__(self):
        # ì˜ì–´ ì•½ì–´ì™€ í•œê¸€ ì„¤ëª… ë§¤í•‘
        self.abbreviations = {
            # ë¶€ë¬¸/ì¡°ì§
            "DX": "ë””ë°”ì´ìŠ¤ê²½í—˜ë¶€ë¬¸",
            "DS": "ë””ë°”ì´ìŠ¤ì†”ë£¨ì…˜ë¶€ë¬¸",
            "MX": "ëª¨ë°”ì¼ê²½í—˜",
            "VD": "ì˜ìƒë””ìŠ¤í”Œë ˆì´",
            
            # ê²½ì˜/ê´€ë¦¬
            "CEO": "ìµœê³ ê²½ì˜ì",
            "CFO": "ìµœê³ ì¬ë¬´ì±…ì„ì",
            "CTO": "ìµœê³ ê¸°ìˆ ì±…ì„ì",
            "CPO": "ìµœê³ ê°œì¸ì •ë³´ë³´í˜¸ì±…ì„ì",
            "CISO": "ìµœê³ ì •ë³´ë³´ì•ˆì±…ì„ì",
            "ESG": "í™˜ê²½ì‚¬íšŒê±°ë²„ë„ŒìŠ¤",
            "CSR": "ê¸°ì—…ì‚¬íšŒì±…ì„",
            "CSV": "ê³µìœ ê°€ì¹˜ì°½ì¶œ",
            
            # í™˜ê²½
            "GHG": "ì˜¨ì‹¤ê°€ìŠ¤",
            "CO2": "ì´ì‚°í™”íƒ„ì†Œ",
            "CO2e": "ì´ì‚°í™”íƒ„ì†Œí™˜ì‚°ëŸ‰",
            "RE100": "ì¬ìƒì—ë„ˆì§€100",
            "ETS": "ë°°ì¶œê¶Œê±°ë˜ì œ",
            "LCA": "ì „ê³¼ì •í‰ê°€",
            "PCF": "ì œí’ˆíƒ„ì†Œë°œìêµ­",
            
            # ê¸°ìˆ 
            "AI": "ì¸ê³µì§€ëŠ¥",
            "ML": "ë¨¸ì‹ ëŸ¬ë‹",
            "IoT": "ì‚¬ë¬¼ì¸í„°ë„·",
            "5G": "5ì„¸ëŒ€ì´ë™í†µì‹ ",
            "SW": "ì†Œí”„íŠ¸ì›¨ì–´",
            "HW": "í•˜ë“œì›¨ì–´",
            "R&D": "ì—°êµ¬ê°œë°œ",
            "IP": "ì§€ì ì¬ì‚°ê¶Œ",
            
            # ë°˜ë„ì²´
            "DRAM": "ë””ë¨",
            "NAND": "ë‚¸ë“œí”Œë˜ì‹œ",
            "SSD": "ì†”ë¦¬ë“œìŠ¤í…Œì´íŠ¸ë“œë¼ì´ë¸Œ",
            "CPU": "ì¤‘ì•™ì²˜ë¦¬ì¥ì¹˜",
            "GPU": "ê·¸ë˜í”½ì²˜ë¦¬ì¥ì¹˜",
            "NPU": "ì‹ ê²½ë§ì²˜ë¦¬ì¥ì¹˜",
            "AP": "ì• í”Œë¦¬ì¼€ì´ì…˜í”„ë¡œì„¸ì„œ",
            
            # êµ­ì œ í‘œì¤€/ì´ë‹ˆì…”í‹°ë¸Œ
            "SDGs": "ì§€ì†ê°€ëŠ¥ë°œì „ëª©í‘œ",
            "TCFD": "ê¸°í›„ë³€í™”ì¬ë¬´ì •ë³´ê³µê°œ",
            "SASB": "ì§€ì†ê°€ëŠ¥íšŒê³„ê¸°ì¤€ìœ„ì›íšŒ",
            "GRI": "ê¸€ë¡œë²Œë³´ê³ ì´ë‹ˆì…”í‹°ë¸Œ",
            "CDP": "íƒ„ì†Œì •ë³´ê³µê°œí”„ë¡œì íŠ¸",
            "UNGC": "ìœ ì—”ê¸€ë¡œë²Œì½¤íŒ©íŠ¸",
            "RBA": "ì±…ì„ìˆëŠ”ë¹„ì¦ˆë‹ˆìŠ¤ì—°í•©",
            "RMI": "ì±…ì„ìˆëŠ”ê´‘ë¬¼ì´ë‹ˆì…”í‹°ë¸Œ",
            "AWS": "êµ­ì œìˆ˜ìì›ê´€ë¦¬ë™ë§¹",
            
            # ê¸°íƒ€
            "M&A": "ì¸ìˆ˜í•©ë³‘",
            "MOU": "ì—…ë¬´í˜‘ì•½",
            "KPI": "í•µì‹¬ì„±ê³¼ì§€í‘œ",
            "CPMS": "ì»´í”Œë¼ì´ì–¸ìŠ¤í”„ë¡œê·¸ë¨ê´€ë¦¬ì‹œìŠ¤í…œ",
            "ERP": "ì „ì‚¬ì ìì›ê´€ë¦¬",
            "SCM": "ê³µê¸‰ë§ê´€ë¦¬",
        }
        
        # ë‹¨ìœ„ ì •ê·œí™” ë§¤í•‘
        self.unit_normalizations = {
            "ì–µì›": "ì–µ ì›",
            "ì¡°ì›": "ì¡° ì›",
            "ë§Œì›": "ë§Œ ì›",
            "ì²œì›": "ì²œ ì›",
            "ë§Œí†¤": "ë§Œ í†¤",
            "ì²œí†¤": "ì²œ í†¤",
            "ë§Œëª…": "ë§Œ ëª…",
            "ì²œëª…": "ì²œ ëª…",
            "í¼ì„¼íŠ¸": "%",
            "í”„ë¡œ": "%"
        }
        
        # ì¤‘ìš” í‚¤ì›Œë“œ ì‚¬ì „
        self.important_keywords = {
            "í™˜ê²½": ["íƒ„ì†Œì¤‘ë¦½", "ë„·ì œë¡œ", "ì¬ìƒì—ë„ˆì§€", "ìì›ìˆœí™˜", "ìˆ˜ìì›", 
                    "ìƒë¬¼ë‹¤ì–‘ì„±", "ìˆœí™˜ê²½ì œ", "ê¸°í›„ë³€í™”", "ì˜¨ì‹¤ê°€ìŠ¤", "ì¹œí™˜ê²½"],
            "ì‚¬íšŒ": ["ì¸ê¶Œ", "ì•ˆì „ë³´ê±´", "ë‹¤ì–‘ì„±", "í¬ìš©ì„±", "ê³µê¸‰ë§", 
                    "í˜‘ë ¥íšŒì‚¬", "ì‚¬íšŒê³µí—Œ", "ì§€ì—­ì‚¬íšŒ", "ì„ì§ì›", "ë…¸ë™"],
            "ê±°ë²„ë„ŒìŠ¤": ["ì´ì‚¬íšŒ", "ì§€ë°°êµ¬ì¡°", "ì¤€ë²•", "ìœ¤ë¦¬ê²½ì˜", "ì»´í”Œë¼ì´ì–¸ìŠ¤",
                      "ë¦¬ìŠ¤í¬ê´€ë¦¬", "íˆ¬ëª…ì„±", "ë°˜ë¶€íŒ¨", "ê³µì •ê±°ë˜", "ì£¼ì£¼"],
            "ì¬ë¬´": ["ë§¤ì¶œ", "ì˜ì—…ì´ìµ", "ìˆœì´ìµ", "ìì‚°", "ë¶€ì±„", 
                   "íˆ¬ì", "ë°°ë‹¹", "ì‹¤ì ", "ì„±ì¥", "ìˆ˜ìµì„±"],
            "ê¸°ìˆ ": ["í˜ì‹ ", "ë””ì§€í„¸ì „í™˜", "ë°˜ë„ì²´", "ì¸ê³µì§€ëŠ¥", "ë¹…ë°ì´í„°",
                   "í´ë¼ìš°ë“œ", "ë³´ì•ˆ", "ê°œì¸ì •ë³´", "ì‚¬ì´ë²„ë³´ì•ˆ", "ë¸”ë¡ì²´ì¸"]
        }
    
    def preprocess(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        # 1. ê¸°ë³¸ ì •ë¦¬
        text = self._clean_basic(text)
        
        # 2. ì˜ì–´ ì•½ì–´ ì²˜ë¦¬
        text = self._process_abbreviations(text)
        
        # 3. ìˆ«ìì™€ ë‹¨ìœ„ ì •ê·œí™”
        text = self._normalize_numbers_and_units(text)
        
        # 4. íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = self._clean_special_chars(text)
        
        return text.strip()
    
    def _clean_basic(self, text: str) -> str:
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        
        # ì—°ì†ëœ ê°œí–‰ì„ ë‘ ê°œë¡œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # íƒ­ì„ ê³µë°±ìœ¼ë¡œ
        text = text.replace('\t', ' ')
        
        return text
    
    def _process_abbreviations(self, text: str) -> str:
        """ì˜ì–´ ì•½ì–´ë¥¼ í•œê¸€ ì„¤ëª…ê³¼ í•¨ê»˜ í‘œê¸°"""
        for abbr, korean in self.abbreviations.items():
            # ì´ë¯¸ ë³‘ê¸°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            pattern = f"{abbr}\\({korean}\\)"
            if pattern in text:
                continue
            
            # ì•½ì–´ë§Œ ìˆëŠ” ê²½ìš° í•œê¸€ ë³‘ê¸° ì¶”ê°€
            # ë‹¨ì–´ ê²½ê³„ë¥¼ í™•ì¸í•˜ì—¬ ì •í™•í•œ ë§¤ì¹­
            text = re.sub(
                rf'\b{re.escape(abbr)}\b(?!\()',
                f'{abbr}({korean})',
                text
            )
        
        return text
    
    def _normalize_numbers_and_units(self, text: str) -> str:
        """ìˆ«ìì™€ ë‹¨ìœ„ ì •ê·œí™”"""
        # ì²œë‹¨ìœ„ êµ¬ë¶„ ì‰¼í‘œ ì •ê·œí™”
        text = re.sub(r'(\d{1,3}),(\d{3})', r'\1,\2', text)
        
        # ë‹¨ìœ„ ì •ê·œí™”
        for old, new in self.unit_normalizations.items():
            text = text.replace(old, new)
        
        # ìˆ«ìì™€ ë‹¨ìœ„ ì‚¬ì´ ê³µë°± ì •ê·œí™”
        # 174ì¡°8,877ì–µì› -> 174ì¡° 8,877ì–µ ì›
        text = re.sub(r'(\d+)ì¡°(\d+)', r'\1ì¡° \2', text)
        text = re.sub(r'(\d+)ì–µ(\d+)', r'\1ì–µ \2', text)
        
        # Scope 1,2,3 ì •ê·œí™”
        text = re.sub(r'Scope\s*1', 'Scope 1(ì§ì ‘ë°°ì¶œ)', text)
        text = re.sub(r'Scope\s*2', 'Scope 2(ê°„ì ‘ë°°ì¶œ)', text)
        text = re.sub(r'Scope\s*3', 'Scope 3(ê¸°íƒ€ê°„ì ‘ë°°ì¶œ)', text)
        
        return text
    
    def _clean_special_chars(self, text: str) -> str:
        """íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬"""
        # ì´ìƒí•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\s\(\)\[\]\{\}.,;:!?\-=+*/%\'"ê°€-í£ã„±-ã…ã…-ã…£]', ' ', text)
        
        # ì—°ì†ëœ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'\.{3,}', '...', text)  # ë§ì¤„ì„í‘œ
        text = re.sub(r'-{3,}', '---', text)   # êµ¬ë¶„ì„ 
        
        return text
    
    def extract_metadata(self, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {
            'numbers': self._extract_numbers(text),
            'keywords': self._extract_keywords(text),
            'entities': self._extract_entities(text),
            'dates': self._extract_dates(text)
        }
        
        return metadata
    
    def _extract_numbers(self, text: str) -> List[Dict]:
        """ì¤‘ìš” ìˆ˜ì¹˜ ì¶”ì¶œ"""
        numbers = []
        
        # ê¸ˆì•¡ ì¶”ì¶œ
        money_pattern = r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(ì¡°|ì–µ|ë§Œ|ì²œ)?\s*ì›'
        for match in re.finditer(money_pattern, text):
            numbers.append({
                'value': match.group(0),
                'type': 'money',
                'unit': 'ì›'
            })
        
        # í¼ì„¼íŠ¸ ì¶”ì¶œ
        percent_pattern = r'(\d+(?:\.\d+)?)\s*%'
        for match in re.finditer(percent_pattern, text):
            numbers.append({
                'value': match.group(0),
                'type': 'percentage',
                'unit': '%'
            })
        
        # ìˆ˜ëŸ‰ ì¶”ì¶œ (í†¤, ëª… ë“±)
        quantity_pattern = r'(\d+(?:,\d{3})*)\s*(í†¤|ëª…|ê°œ|ê±´|íšŒ)'
        for match in re.finditer(quantity_pattern, text):
            numbers.append({
                'value': match.group(0),
                'type': 'quantity',
                'unit': match.group(2)
            })
        
        return numbers
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        found_keywords = []
        
        for category, keywords in self.important_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    found_keywords.append(keyword)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(found_keywords))
    
    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """ê°œì²´ëª… ì¶”ì¶œ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)"""
        entities = {
            'organization': [],
            'product': [],
            'location': []
        }
        
        # ì¡°ì§ëª…
        org_patterns = [
            r'ì‚¼ì„±ì „ì(?:ì£¼ì‹íšŒì‚¬)?',
            r'DXë¶€ë¬¸|DSë¶€ë¬¸',
            r'\w+ì‚¬ì—…ë¶€',
            r'\w+ì„¼í„°'
        ]
        for pattern in org_patterns:
            matches = re.findall(pattern, text)
            entities['organization'].extend(matches)
        
        # ì œí’ˆëª…
        product_patterns = [
            r'ê°¤ëŸ­ì‹œ\s*\w+',
            r'ì—‘ì‹œë…¸ìŠ¤\s*\d+',
            r'\w+\s*í”„ë¡œì„¸ì„œ'
        ]
        for pattern in product_patterns:
            matches = re.findall(pattern, text)
            entities['product'].extend(matches)
        
        # ì§€ì—­ëª…
        location_patterns = [
            r'(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)',
            r'(ë¯¸êµ­|ì¤‘êµ­|ì¼ë³¸|ìœ ëŸ½|ì•„ì‹œì•„|ë² íŠ¸ë‚¨|ì¸ë„)',
            r'(ê¸°í¥|í™”ì„±|í‰íƒ|ì²œì•ˆ|ì˜¨ì–‘|êµ¬ë¯¸|ê´‘ì£¼)(?:ì‚¬ì—…ì¥|ìº í¼ìŠ¤)?'
        ]
        for pattern in location_patterns:
            matches = re.findall(pattern, text)
            entities['location'].extend(matches)
        
        # ì¤‘ë³µ ì œê±°
        for key in entities:
            entities[key] = list(set(entities[key]))
        
        return entities
    
    def _extract_dates(self, text: str) -> List[str]:
        """ë‚ ì§œ/ì—°ë„ ì¶”ì¶œ"""
        dates = []
        
        # ì—°ë„
        year_pattern = r'20\d{2}ë…„'
        dates.extend(re.findall(year_pattern, text))
        
        # ë‚ ì§œ
        date_pattern = r'\d{1,2}ì›”\s*\d{1,2}ì¼'
        dates.extend(re.findall(date_pattern, text))
        
        # ë¶„ê¸°
        quarter_pattern = r'\d{1}ë¶„ê¸°|\d{1}Q'
        dates.extend(re.findall(quarter_pattern, text))
        
        return list(set(dates))
    
    def create_search_friendly_text(self, text: str) -> str:
        """ê²€ìƒ‰ ìµœì í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±"""
        # ì „ì²˜ë¦¬
        processed = self.preprocess(text)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata = self.extract_metadata(processed)
        
        # í‚¤ì›Œë“œë¥¼ í…ìŠ¤íŠ¸ ëì— ì¶”ê°€ (ê²€ìƒ‰ í–¥ìƒ)
        if metadata['keywords']:
            keyword_text = ' [í‚¤ì›Œë“œ: ' + ', '.join(metadata['keywords']) + ']'
            processed += keyword_text
        
        return processed
    
    def normalize_query(self, query: str) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ì •ê·œí™”"""
        # ê¸°ë³¸ ì „ì²˜ë¦¬
        query = self._clean_basic(query)
        
        # ì•½ì–´ í™•ì¥
        for abbr, korean in self.abbreviations.items():
            if abbr.lower() in query.lower():
                query += f" {korean}"
        
        # ë™ì˜ì–´ í™•ì¥
        synonyms = {
            "ë§¤ì¶œ": "ë§¤ì¶œ ìˆ˜ìµ ì‹¤ì  ì˜ì—…ìˆ˜ìµ",
            "ì´ìµ": "ì´ìµ ì˜ì—…ì´ìµ ìˆœì´ìµ",
            "í™˜ê²½": "í™˜ê²½ ESG ì§€ì†ê°€ëŠ¥ ì¹œí™˜ê²½",
            "íƒ„ì†Œ": "íƒ„ì†Œ ì˜¨ì‹¤ê°€ìŠ¤ CO2 ë°°ì¶œ",
            "ì„ì§ì›": "ì„ì§ì› ì§ì› ì¢…ì—…ì› ê·¼ë¡œì"
        }
        
        for key, expansion in synonyms.items():
            if key in query:
                query = expansion
                break
        
        return query.strip()


def test_preprocessor():
    """ì „ì²˜ë¦¬ê¸° í…ŒìŠ¤íŠ¸"""
    preprocessor = TextPreprocessor()
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_texts = [
        "DXë¶€ë¬¸ì€ 2030ë…„ê¹Œì§€ Scope 1, 2 íƒ„ì†Œì¤‘ë¦½ì„ ë‹¬ì„±í•  ê³„íšì…ë‹ˆë‹¤.",
        "2024ë…„ ë§¤ì¶œì€ 174ì¡°8,877ì–µì›ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.",
        "CEOëŠ” ESG ê²½ì˜ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í–ˆìŠµë‹ˆë‹¤.",
        "ì¬ìƒì—ë„ˆì§€ ì „í™˜ìœ¨ì´ 93.4%ì— ë‹¬í–ˆìŠµë‹ˆë‹¤."
    ]
    
    print("ğŸ“ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸:\n")
    
    for i, text in enumerate(test_texts, 1):
        print(f"[ì›ë³¸ {i}] {text}")
        processed = preprocessor.preprocess(text)
        print(f"[ì²˜ë¦¬ {i}] {processed}")
        
        metadata = preprocessor.extract_metadata(processed)
        print(f"[ë©”íƒ€ {i}] í‚¤ì›Œë“œ: {metadata['keywords']}")
        print(f"         ìˆ˜ì¹˜: {[n['value'] for n in metadata['numbers']]}")
        print()
    
    # ì¿¼ë¦¬ ì •ê·œí™” í…ŒìŠ¤íŠ¸
    print("\nğŸ” ì¿¼ë¦¬ ì •ê·œí™” í…ŒìŠ¤íŠ¸:\n")
    
    test_queries = [
        "DX ë§¤ì¶œ",
        "íƒ„ì†Œì¤‘ë¦½ ëª©í‘œ",
        "CEO ë©”ì‹œì§€"
    ]
    
    for query in test_queries:
        normalized = preprocessor.normalize_query(query)
        print(f"[ì›ë³¸] {query}")
        print(f"[ì •ê·œí™”] {normalized}\n")


if __name__ == "__main__":
    test_preprocessor()