#!/usr/bin/env python3
"""
í•œêµ­ì–´ ìµœì í™” ë²¡í„° DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸
ko-sroberta ëª¨ë¸ê³¼ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ì„ ì‚¬ìš©í•œ ê³ í’ˆì§ˆ ì„ë² ë”©
"""

import os
import sys
from pathlib import Path
import shutil
import time
from typing import List, Dict

# ëª¨ë“ˆ ì„í¬íŠ¸
from korean_vector_store import KoreanVectorStore
from smart_chunker import SmartChunker
from text_preprocessor import TextPreprocessor


def build_korean_vectordb():
    """í•œêµ­ì–´ ìµœì í™” ë²¡í„° DB êµ¬ì¶•"""
    
    print("=" * 60)
    print("ğŸš€ í•œêµ­ì–´ ìµœì í™” ë²¡í„° DB êµ¬ì¶• ì‹œì‘")
    print("=" * 60)
    
    # ê²½ë¡œ ì„¤ì •
    markdown_path = Path("/Users/donghyunkim/Desktop/joo_project/samsung_chatbot/data/samsung_esg_advanced.md")
    db_path = Path("/Users/donghyunkim/Desktop/joo_project/samsung_chatbot/data/chroma_db_korean")
    
    # íŒŒì¼ í™•ì¸
    if not markdown_path.exists():
        print(f"âŒ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {markdown_path}")
        return False
    
    print(f"ğŸ“„ ì…ë ¥ íŒŒì¼: {markdown_path}")
    print(f"ğŸ’¾ ì¶œë ¥ ê²½ë¡œ: {db_path}")
    
    # ê¸°ì¡´ DB ì‚­ì œ
    if db_path.exists():
        print("\nğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ ì¤‘...")
        shutil.rmtree(db_path)
        print("âœ… ê¸°ì¡´ DB ì‚­ì œ ì™„ë£Œ")
    
    # 1ë‹¨ê³„: ì²­í‚¹
    print("\n" + "=" * 60)
    print("ğŸ“ 1ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹")
    print("-" * 60)
    
    start_time = time.time()
    chunker = SmartChunker(chunk_size=1200, chunk_overlap=300)
    chunks = chunker.chunk_markdown(markdown_path)
    
    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
    
    # ì²­í¬ í†µê³„
    print_chunk_statistics(chunks)
    
    # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    print("\n" + "=" * 60)
    print("ğŸ”§ 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬")
    print("-" * 60)
    
    start_time = time.time()
    preprocessor = TextPreprocessor()
    
    processed_chunks = []
    for chunk in chunks:
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_text = preprocessor.preprocess(chunk['content'])
        
        # ë©”íƒ€ë°ì´í„° ë³´ê°• (ChromaDBëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜)
        extracted_metadata = preprocessor.extract_metadata(processed_text)
        
        # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        metadata = chunk['metadata'].copy()
        
        # ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ í•„ë“œ ì²˜ë¦¬
        if 'metrics' in metadata and isinstance(metadata['metrics'], list):
            metadata['metrics'] = ', '.join(metadata['metrics']) if metadata['metrics'] else ''
        if 'keywords' in metadata and isinstance(metadata['keywords'], list):
            metadata['keywords'] = ', '.join(metadata['keywords']) if metadata['keywords'] else ''
        if 'years' in metadata and isinstance(metadata['years'], list):
            metadata['years'] = ', '.join(metadata['years']) if metadata['years'] else ''
        
        # ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        metadata['extracted_keywords'] = ', '.join(extracted_metadata['keywords']) if extracted_metadata['keywords'] else ''
        metadata['extracted_numbers'] = ', '.join([n['value'] for n in extracted_metadata['numbers']]) if extracted_metadata['numbers'] else ''
        metadata['extracted_dates'] = ', '.join(extracted_metadata['dates']) if extracted_metadata['dates'] else ''
        
        processed_chunks.append({
            'content': processed_text,
            'metadata': metadata
        })
    
    print(f"âœ… {len(processed_chunks)}ê°œ ì²­í¬ ì „ì²˜ë¦¬ ì™„ë£Œ")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
    
    # 3ë‹¨ê³„: ë²¡í„° DB êµ¬ì¶•
    print("\n" + "=" * 60)
    print("ğŸ—„ï¸ 3ë‹¨ê³„: ë²¡í„° DB êµ¬ì¶• (ko-sroberta)")
    print("-" * 60)
    
    start_time = time.time()
    
    # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
    vector_store = KoreanVectorStore(persist_directory=str(db_path))
    
    # í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ë¶„ë¦¬
    texts = [chunk['content'] for chunk in processed_chunks]
    metadatas = [chunk['metadata'] for chunk in processed_chunks]
    
    # ë²¡í„° DBì— ì¶”ê°€
    vector_store.add_documents(texts, metadatas)
    
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
    
    # 4ë‹¨ê³„: ê²€ì¦
    print("\n" + "=" * 60)
    print("ğŸ” 4ë‹¨ê³„: ë²¡í„° DB ê²€ì¦")
    print("-" * 60)
    
    # DB í†µê³„
    stats = vector_store.get_statistics()
    print(f"\nğŸ“Š ë²¡í„° DB í†µê³„:")
    print(f"  ì´ ë¬¸ì„œ ìˆ˜: {stats['total_documents']}")
    print(f"  ê³ ìœ  í˜ì´ì§€: {stats['unique_pages']}")
    print(f"  ê³ ìœ  ì„¹ì…˜: {stats['unique_sections']}")
    print(f"  ì„ë² ë”© ì°¨ì›: {stats['embedding_dimension']}")
    
    if stats.get('chunk_types'):
        print(f"\n  ì²­í¬ íƒ€ì… ë¶„í¬:")
        for ctype, count in stats['chunk_types'].items():
            print(f"    {ctype}: {count}ê°œ")
    
    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    print("\n" + "=" * 60)
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
    print("-" * 60)
    
    test_queries = [
        "2024ë…„ ë§¤ì¶œ ì‹¤ì ",
        "DXë¶€ë¬¸ íƒ„ì†Œì¤‘ë¦½ ëª©í‘œ",
        "ì¬ìƒì—ë„ˆì§€ ì „í™˜ìœ¨",
        "CEO ë©”ì‹œì§€",
        "DSë¶€ë¬¸ ë°˜ë„ì²´ ì‚¬ì—…",
        "ESG ì „ëµ",
        "í˜‘ë ¥íšŒì‚¬ ì§€ì›",
        "ì„ì§ì› ë³µì§€"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” ê²€ìƒ‰: '{query}'")
        results = vector_store.similarity_search(query, k=3)
        
        if results:
            for i, doc in enumerate(results, 1):
                print(f"\n  [{i}] í˜ì´ì§€ {doc.metadata.get('page', 'N/A')}, "
                      f"ì„¹ì…˜: {doc.metadata.get('section', 'N/A')}")
                print(f"      {doc.page_content[:150]}...")
                
                # ìˆ˜ì¹˜ ì •ë³´ê°€ ìˆìœ¼ë©´ í‘œì‹œ
                if doc.metadata.get('metrics'):
                    print(f"      ğŸ“Š ìˆ˜ì¹˜: {', '.join(doc.metadata['metrics'][:3])}")
        else:
            print("  âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
    
    print("\n" + "=" * 60)
    print("âœ… í•œêµ­ì–´ ìµœì í™” ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {db_path}")
    print("=" * 60)
    
    return True


def print_chunk_statistics(chunks: List[Dict]):
    """ì²­í¬ í†µê³„ ì¶œë ¥"""
    # íƒ€ì…ë³„ í†µê³„
    type_stats = {}
    section_stats = {}
    page_stats = {}
    
    total_chars = 0
    metrics_count = 0
    keywords_count = 0
    
    for chunk in chunks:
        metadata = chunk['metadata']
        
        # íƒ€ì… í†µê³„
        chunk_type = metadata.get('chunk_type', 'unknown')
        type_stats[chunk_type] = type_stats.get(chunk_type, 0) + 1
        
        # ì„¹ì…˜ í†µê³„
        section = metadata.get('section', 'unknown')
        section_stats[section] = section_stats.get(section, 0) + 1
        
        # í˜ì´ì§€ í†µê³„
        page = metadata.get('page', 0)
        if page not in page_stats:
            page_stats[page] = 0
        page_stats[page] += 1
        
        # ë¬¸ì ìˆ˜
        total_chars += len(chunk['content'])
        
        # ë©”íŠ¸ë¦­ê³¼ í‚¤ì›Œë“œ
        if metadata.get('metrics'):
            metrics_count += len(metadata['metrics'])
        if metadata.get('keywords'):
            keywords_count += len(metadata['keywords'])
    
    print(f"\nğŸ“ˆ ì²­í¬ í†µê³„:")
    print(f"  í‰ê·  ì²­í¬ í¬ê¸°: {total_chars // len(chunks)}ì")
    print(f"  ì´ ìˆ˜ì¹˜ ì •ë³´: {metrics_count}ê°œ")
    print(f"  ì´ í‚¤ì›Œë“œ: {keywords_count}ê°œ")
    
    print(f"\n  íƒ€ì…ë³„ ë¶„í¬:")
    for ctype, count in sorted(type_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
        percentage = (count / len(chunks)) * 100
        print(f"    {ctype}: {count}ê°œ ({percentage:.1f}%)")
    
    print(f"\n  ì„¹ì…˜ë³„ ë¶„í¬:")
    for section, count in sorted(section_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
        percentage = (count / len(chunks)) * 100
        print(f"    {section}: {count}ê°œ ({percentage:.1f}%)")
    
    print(f"\n  í˜ì´ì§€ ë²”ìœ„: {min(page_stats.keys())} ~ {max(page_stats.keys())}")


if __name__ == "__main__":
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸
    try:
        import torch
        import transformers
        import chromadb
    except ImportError as e:
        print(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
        print("\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("pip install torch transformers chromadb")
        sys.exit(1)
    
    # ì‹¤í–‰
    success = build_korean_vectordb()
    
    if success:
        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. app_gemini.pyë¥¼ ìˆ˜ì •í•˜ì—¬ ìƒˆ ë²¡í„° DB ì‚¬ìš©")
        print("2. korean_vector_store.pyë¥¼ importí•˜ë„ë¡ ë³€ê²½")
        print("3. ì±—ë´‡ì„ ì‹¤í–‰í•˜ì—¬ ê°œì„ ëœ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    
    sys.exit(0 if success else 1)