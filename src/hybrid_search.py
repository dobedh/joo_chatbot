#!/usr/bin/env python3
"""
í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
BM25 (Sparse) + Dense Retrieval ê²°í•©
"""

from typing import List, Dict, Optional, Tuple
import numpy as np
from rank_bm25 import BM25Okapi
from langchain.schema import Document
import re
import math


class HybridSearch:
    def __init__(self, vector_store, dense_weight: float = 0.6):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì´ˆê¸°í™”
        
        Args:
            vector_store: ê¸°ì¡´ í•œêµ­ì–´ ë²¡í„° ìŠ¤í† ì–´
            dense_weight: Dense ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (0~1)
        """
        self.vector_store = vector_store
        self.dense_weight = dense_weight
        self.sparse_weight = 1 - dense_weight
        
        # BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
        self._build_bm25_index()
    
    def _build_bm25_index(self):
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("ğŸ”„ BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        all_data = self.vector_store.collection.get()
        
        if not all_data or not all_data.get('documents'):
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        self.documents = all_data['documents']
        self.metadatas = all_data['metadatas']
        self.ids = all_data['ids']
        
        # í† í°í™”ëœ ë¬¸ì„œ ìƒì„±
        tokenized_docs = []
        for doc in self.documents:
            tokens = self._tokenize(doc)
            tokenized_docs.append(tokens)
        
        # BM25 ì¸ë±ìŠ¤ ìƒì„±
        self.bm25 = BM25Okapi(tokenized_docs)
        
        print(f"âœ… BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ ({len(self.documents)}ê°œ ë¬¸ì„œ)")
    
    def _tokenize(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í† í°í™”"""
        # ì†Œë¬¸ì ë³€í™˜
        text = text.lower()
        
        # í•œê¸€, ì˜ì–´, ìˆ«ìë§Œ ì¶”ì¶œ
        tokens = re.findall(r'[ê°€-í£]+|[a-z]+|[0-9]+\.?[0-9]*%?', text)
        
        # ë¶ˆìš©ì–´ ì œê±° (ì„ íƒì )
        stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼'}
        tokens = [t for t in tokens if t not in stopwords]
        
        return tokens
    
    def search(
        self, 
        query: str, 
        k: int = 5,
        rerank: bool = True
    ) -> List[Document]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            rerank: ì¬ìˆœìœ„ ì ìš© ì—¬ë¶€
        
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        # ë” ë§ì€ í›„ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ì¬ìˆœìœ„
        k_candidates = k * 3 if rerank else k
        
        # Dense ê²€ìƒ‰ (ì˜ë¯¸ì  ìœ ì‚¬ë„)
        dense_results = self._dense_search(query, k_candidates)
        
        # Sparse ê²€ìƒ‰ (í‚¤ì›Œë“œ ë§¤ì¹­)
        sparse_results = self._sparse_search(query, k_candidates)
        
        # ê²°ê³¼ í†µí•© ë° ì¬ìˆœìœ„
        combined_results = self._combine_results(
            dense_results, 
            sparse_results,
            k
        )
        
        return combined_results
    
    def _dense_search(self, query: str, k: int) -> List[Tuple[Document, float]]:
        """Dense ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„)"""
        # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš©
        docs = self.vector_store.similarity_search(query, k=k)
        
        # ì ìˆ˜ì™€ í•¨ê»˜ ë°˜í™˜ (ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜)
        results = []
        for doc in docs:
            # ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ìŒ
            distance = doc.metadata.get('distance', 0)
            similarity = 1 / (1 + distance)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
            results.append((doc, similarity))
        
        return results
    
    def _sparse_search(self, query: str, k: int) -> List[Tuple[Document, float]]:
        """Sparse ê²€ìƒ‰ (BM25)"""
        # ì¿¼ë¦¬ í† í°í™”
        query_tokens = self._tokenize(query)
        
        # BM25 ì ìˆ˜ ê³„ì‚°
        scores = self.bm25.get_scores(query_tokens)
        
        # ìƒìœ„ kê°œ ì„ íƒ
        top_indices = np.argsort(scores)[::-1][:k]
        
        results = []
        for idx in top_indices:
            if scores[idx] > 0:  # ì ìˆ˜ê°€ 0ë³´ë‹¤ í° ê²½ìš°ë§Œ
                doc = Document(
                    page_content=self.documents[idx],
                    metadata=self.metadatas[idx] if idx < len(self.metadatas) else {}
                )
                # BM25 ì ìˆ˜ ì •ê·œí™”
                normalized_score = scores[idx] / (scores[idx] + 1)
                results.append((doc, normalized_score))
        
        return results
    
    def _combine_results(
        self, 
        dense_results: List[Tuple[Document, float]],
        sparse_results: List[Tuple[Document, float]],
        k: int
    ) -> List[Document]:
        """Denseì™€ Sparse ê²°ê³¼ í†µí•©"""
        # ë¬¸ì„œë³„ ì ìˆ˜ ì§‘ê³„
        doc_scores = {}
        
        # Dense ê²°ê³¼ ì²˜ë¦¬
        for doc, score in dense_results:
            key = doc.page_content[:100]  # ë¬¸ì„œ ì‹ë³„ í‚¤
            if key not in doc_scores:
                doc_scores[key] = {
                    'doc': doc,
                    'dense_score': 0,
                    'sparse_score': 0
                }
            doc_scores[key]['dense_score'] = score
        
        # Sparse ê²°ê³¼ ì²˜ë¦¬
        for doc, score in sparse_results:
            key = doc.page_content[:100]
            if key not in doc_scores:
                doc_scores[key] = {
                    'doc': doc,
                    'dense_score': 0,
                    'sparse_score': 0
                }
            doc_scores[key]['sparse_score'] = score
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        final_scores = []
        for key, scores in doc_scores.items():
            combined_score = (
                self.dense_weight * scores['dense_score'] +
                self.sparse_weight * scores['sparse_score']
            )
            final_scores.append((scores['doc'], combined_score))
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        final_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ kê°œ ë°˜í™˜
        return [doc for doc, _ in final_scores[:k]]
    
    def search_with_filter(
        self,
        query: str,
        k: int = 5,
        filter_dict: Optional[Dict] = None
    ) -> List[Document]:
        """í•„í„°ë¥¼ ì ìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        # ê¸°ë³¸ ê²€ìƒ‰
        all_results = self.search(query, k=k*2)  # ë” ë§ì´ ê°€ì ¸ì˜¨ í›„ í•„í„°ë§
        
        if not filter_dict:
            return all_results[:k]
        
        # í•„í„° ì ìš©
        filtered = []
        for doc in all_results:
            match = True
            for key, value in filter_dict.items():
                if doc.metadata.get(key) != value:
                    match = False
                    break
            if match:
                filtered.append(doc)
        
        return filtered[:k]


def test_hybrid_search():
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    from pathlib import Path
    from korean_vector_store import KoreanVectorStore
    
    print("=" * 70)
    print("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
    korean_db_path = Path("/Users/donghyunkim/Desktop/joo_project/samsung_chatbot/data/chroma_db_korean")
    vector_store = KoreanVectorStore(persist_directory=str(korean_db_path))
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì´ˆê¸°í™”
    hybrid = HybridSearch(vector_store, dense_weight=0.6)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "ì¸ê¶Œ êµìœ¡ì„ ëª‡í”„ë¡œê°€ ë°›ì•˜ì–´?",
        "HRA",
        "ì¸ê¶Œ ì±”í”¼ì–¸",
        "DXë¶€ë¬¸ íƒ„ì†Œì¤‘ë¦½"
    ]
    
    for query in test_queries:
        print(f"\nğŸ“ ì¿¼ë¦¬: '{query}'")
        print("-" * 50)
        
        results = hybrid.search(query, k=3)
        
        for i, doc in enumerate(results, 1):
            page = doc.metadata.get('page', 'N/A')
            section = doc.metadata.get('section', 'N/A')
            
            # ì£¼ìš” í‚¤ì›Œë“œ í™•ì¸
            content = doc.page_content
            highlights = []
            
            if '95.7%' in content:
                highlights.append('95.7%')
            if 'HRA' in content or 'Human Rights Risk Assessment' in content:
                highlights.append('HRA')
            if 'ì¸ê¶Œ ì±”í”¼ì–¸' in content:
                highlights.append('ì¸ê¶Œ ì±”í”¼ì–¸')
            
            print(f"\n[{i}] í˜ì´ì§€ {page}, ì„¹ì…˜: {section}")
            if highlights:
                print(f"âœ… í¬í•¨: {', '.join(highlights)}")
            print(f"ë‚´ìš©: {content[:150]}...")
    
    print("\n" + "=" * 70)
    print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    test_hybrid_search()