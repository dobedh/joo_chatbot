#!/usr/bin/env python3
"""
ë²¡í„° DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸
"""

from pathlib import Path
from gemini_vector_store import VectorStore
import re
from typing import List, Dict


def create_chunks(markdown_path: Path) -> List[Dict]:
    """ë§ˆí¬ë‹¤ìš´ì„ ì²­í¬ë¡œ ë¶„í• """
    
    with open(markdown_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    chunks = []
    
    # í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¶„í• 
    pages = content.split('## í˜ì´ì§€')
    
    for page_content in pages[1:]:  # ì²« ë²ˆì§¸ëŠ” í—¤ë”
        # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
        page_match = re.match(r' (\d+)', page_content)
        if not page_match:
            continue
        
        page_num = page_match.group(1)
        
        # ë‚´ìš©ì„ ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í•  (600ì ë‹¨ìœ„ - ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸)
        text = page_content[page_match.end():]
        chunk_size = 600
        
        # ì„¹ì…˜ë³„ë¡œ ë¶„ë¦¬ (### ê¸°ì¤€)
        sections = text.split('###')
        
        for section in sections:
            if not section.strip():
                continue
                
            # ì„¹ì…˜ì´ ë„ˆë¬´ ê¸¸ë©´ ì²­í¬ë¡œ ë¶„í• 
            if len(section) > chunk_size:
                for i in range(0, len(section), chunk_size):
                    chunk_text = section[i:i+chunk_size].strip()
                    
                    if len(chunk_text) > 50:  # ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ë§Œ
                        chunks.append({
                            'content': chunk_text,
                            'metadata': {
                                'page': int(page_num),
                                'chunk_start': i,
                                'chunk_end': min(i+chunk_size, len(section)),
                                'source': 'samsung_esg_advanced.md'
                            }
                        })
            else:
                # ì„¹ì…˜ì´ ì ë‹¹í•œ í¬ê¸°ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                if len(section.strip()) > 50:
                    chunks.append({
                        'content': section.strip(),
                        'metadata': {
                            'page': int(page_num),
                            'chunk_start': 0,
                            'chunk_end': len(section),
                            'source': 'samsung_esg_advanced.md'
                        }
                    })
    
    return chunks


def main():
    # ê²½ë¡œ ì„¤ì •
    markdown_path = Path("/Users/donghyunkim/Desktop/joo_project/samsung_chatbot/data/samsung_esg_advanced.md")
    db_path = Path("/Users/donghyunkim/Desktop/joo_project/samsung_chatbot/data/chroma_db")
    
    # ê¸°ì¡´ DB ì‚­ì œ
    if db_path.exists():
        import shutil
        shutil.rmtree(db_path)
        print("ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ ì™„ë£Œ")
    
    # ì²­í¬ ìƒì„±
    print("ğŸ“ ì²­í¬ ìƒì„± ì¤‘...")
    chunks = create_chunks(markdown_path)
    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
    
    # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
    print("ğŸš€ ë²¡í„° DB êµ¬ì¶• ì¤‘...")
    vector_store = VectorStore(persist_directory=str(db_path))
    
    # ë¬¸ì„œ ì¶”ê°€
    texts = [chunk['content'] for chunk in chunks]
    metadatas = [chunk['metadata'] for chunk in chunks]
    
    vector_store.add_documents(texts, metadatas)
    print(f"âœ… ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ: {db_path}")
    
    # ìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
    test_queries = [
        "ì‚¼ì„±ì „ì ë§¤ì¶œ ì˜ì—…ì´ìµ",
        "íƒ„ì†Œì¤‘ë¦½ ëª©í‘œ",
        "ì¬ìƒì—ë„ˆì§€ ì „í™˜ìœ¨",
        "CEO ë©”ì‹œì§€",
        "DSë¶€ë¬¸ ì‹¤ì "
    ]
    
    for query in test_queries:
        print(f"\nì§ˆë¬¸: {query}")
        results = vector_store.similarity_search(query, k=2)
        for i, doc in enumerate(results):
            print(f"  ê²°ê³¼ {i+1} (í˜ì´ì§€ {doc.metadata.get('page', 'N/A')}):")
            print(f"    {doc.page_content[:100]}...")
    
    return db_path


if __name__ == "__main__":
    db_path = main()
    print(f"\nğŸ¯ ì™„ë£Œ! ë²¡í„° DB ìœ„ì¹˜: {db_path}")