#!/usr/bin/env python3
"""
ë²¡í„° DB ë‚´ìš© ê²€ì‚¬ ìŠ¤í¬ë¦½íŠ¸
ì €ì¥ëœ ë°ì´í„°ì˜ í˜•íƒœì™€ ë‚´ìš©ì„ ìì„¸íˆ í™•ì¸
"""

import sys
import json
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))

from src.config import *
from src.gemini_vector_store import GeminiVectorStore

def inspect_vector_db():
    print("ğŸ” ë²¡í„° DB ë‚´ìš© ê²€ì‚¬ ì‹œì‘...")
    print("=" * 60)
    
    # Initialize vector store
    vector_store = GeminiVectorStore(
        persist_directory=CHROMA_PERSIST_DIRECTORY
    )
    
    if not vector_store.exists():
        print("âŒ ë²¡í„° DBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        return
    
    # 1. ê¸°ë³¸ ì •ë³´
    doc_count = vector_store.collection.count()
    print(f"ğŸ“Š ì´ ë¬¸ì„œ ìˆ˜: {doc_count}")
    print("-" * 60)
    
    # 2. ì²« 5ê°œ ë¬¸ì„œ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
    print("ğŸ“ ë¬¸ì„œ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):")
    print("-" * 60)
    
    # Get all data (limited to first 5)
    all_data = vector_store.collection.get(
        limit=5,
        include=['documents', 'metadatas', 'embeddings']
    )
    
    for i, (doc_id, document, metadata) in enumerate(zip(
        all_data['ids'], 
        all_data['documents'], 
        all_data['metadatas']
    )):
        print(f"\nğŸ“„ ë¬¸ì„œ {i+1}:")
        print(f"   ID: {doc_id}")
        print(f"   ë©”íƒ€ë°ì´í„°: {json.dumps(metadata, ensure_ascii=False, indent=2)}")
        print(f"   ë‚´ìš© ê¸¸ì´: {len(document)} ë¬¸ì")
        print(f"   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
        print(f"   ã€Œ{document[:300]}...ã€")
        
        # ì„ë² ë”© ì •ë³´ (ì°¨ì›ë§Œ í™•ì¸)
        try:
            if all_data['embeddings']:
                embedding_dim = len(all_data['embeddings'][i]) if i < len(all_data['embeddings']) else 0
                print(f"   ì„ë² ë”© ì°¨ì›: {embedding_dim}")
        except:
            print(f"   ì„ë² ë”©: ì •ë³´ í™•ì¸ ë¶ˆê°€")
        
        print("-" * 40)
    
    # 3. í˜ì´ì§€ë³„ ë¶„í¬ í™•ì¸
    print("\nğŸ“Š í˜ì´ì§€ë³„ ë¬¸ì„œ ë¶„í¬:")
    print("-" * 60)
    
    # Get all metadata
    all_metadata = vector_store.collection.get(
        include=['metadatas']
    )['metadatas']
    
    # Count by page
    page_counts = {}
    for metadata in all_metadata:
        page = metadata.get('page', 'Unknown')
        page_counts[page] = page_counts.get(page, 0) + 1
    
    # Sort by page number
    sorted_pages = sorted(page_counts.items(), key=lambda x: (str(x[0]).isnumeric(), int(x[0]) if str(x[0]).isnumeric() else float('inf'), x[0]))
    
    for page, count in sorted_pages[:20]:  # Show first 20 pages
        print(f"   í˜ì´ì§€ {page}: {count}ê°œ ì²­í¬")
    
    if len(sorted_pages) > 20:
        print(f"   ... ì´ {len(sorted_pages)}ê°œ í˜ì´ì§€")
    
    # 4. ë¬¸ì„œ ê¸¸ì´ í†µê³„
    print(f"\nğŸ“ ë¬¸ì„œ ê¸¸ì´ í†µê³„:")
    print("-" * 60)
    
    all_docs = vector_store.collection.get(include=['documents'])['documents']
    doc_lengths = [len(doc) for doc in all_docs]
    
    print(f"   í‰ê·  ê¸¸ì´: {sum(doc_lengths) / len(doc_lengths):.1f} ë¬¸ì")
    print(f"   ìµœì†Œ ê¸¸ì´: {min(doc_lengths)} ë¬¸ì")
    print(f"   ìµœëŒ€ ê¸¸ì´: {max(doc_lengths)} ë¬¸ì")
    
    # 5. í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    print("-" * 60)
    
    test_keywords = ["ì‚¼ì„±ì „ì", "ESG", "íƒ„ì†Œ", "í™˜ê²½", "ì§€ì†ê°€ëŠ¥"]
    
    for keyword in test_keywords:
        # Simple text search in documents
        matching_docs = []
        for i, doc in enumerate(all_docs):
            if keyword in doc:
                matching_docs.append(i)
        
        print(f"   '{keyword}' í¬í•¨ ë¬¸ì„œ: {len(matching_docs)}ê°œ")
        
        if matching_docs:
            # Show one example
            sample_idx = matching_docs[0]
            sample_metadata = all_metadata[sample_idx]
            print(f"     ì˜ˆì‹œ: í˜ì´ì§€ {sample_metadata.get('page', 'Unknown')}")
    
    print("\nâœ… ë²¡í„° DB ê²€ì‚¬ ì™„ë£Œ!")

if __name__ == "__main__":
    inspect_vector_db()